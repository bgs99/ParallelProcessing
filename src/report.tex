\documentclass[14pt, a4paper, oneside, final]{extarticle}

\usepackage{cmap}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage{geometry} 
\usepackage{graphicx, caption}
\usepackage{amssymb, amsmath, amsfonts}
\usepackage{xcolor, hyperref}
\usepackage{setspace, fullpage, indentfirst}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{csvsimple}

\addto\captionsrussian{\def\refname{СПИСОК ИСПОЛЬЗОВАННЫХ ИСТОЧНИКОВ}}

\geometry{left=3cm, top=2cm, right=1.5cm, bottom=2cm}
\geometry{nohead, includefoot}
\geometry{foot=4mm, footskip=8mm}
\captionsetup{labelsep=period, justification=centering}
\setlength{\parindent}{1.27cm}
\doublespacing
\emergencystretch=25pt


\makeatletter
\makeatother

% НАЧАЛО ТИТУЛЬНОГО ЛИСТА 
\begin{document} 
\setcounter{page}{0}
\begin{center} 
\small
\footnotesize{ФЕДЕРАЛЬНОЕ ГОСУДАРСТВЕННОЕ АВТОНОМНОЕ ОБРАЗОВАТЕЛЬНОЕ}\\
\footnotesize{УЧРЕЖДЕНИЕ ВЫСШЕГО ОБРАЗОВАНИЯ}\\ 
\footnotesize{«НАЦИОНАЛЬНЫЙ ИССЛЕДОВАТЕЛЬСКИЙ УНИВЕРСИТЕТ ИТМО»}\\
\hfill \break 
\footnotesize{ФАКУЛЬТЕТ ПИИКТ}\\
\hfill \break
\hfill \break 
\hfill \break
\large{
    \textbf{Отчет по лабораторной работе №3}

    \textbf{по дисциплине Параллельные вычисления}
}

\hfill \break 
\end{center} 
\begin{flushright} 
Выполнили студенты\\
Гуляев Б. С., P42141\\
Аргынова К. А., P42191\\
\end{flushright}
\vspace*{\fill}
\begin{center}
Санкт-Петербург

2022
\end{center}
\normalsize
\thispagestyle{empty} 
\clearpage
\def\contentsname{ОГЛАВЛЕНИЕ}
\tableofcontents 

\clearpage
\section*{ВВЕДЕНИЕ}
\subsection*{Цель работы}
Получить опыт работы с OpenMP.
\subsection*{Задачи}
\begin{enumerate}
 \item Добавить во все for-циклы (кроме цикла в функции $main$, указывающего количество экспериментов) в программе из ЛР №1 следующую директиву OpenMP:
\begin{lstlisting}[language=C]
#pragma omp parallel for default(none)
        private(...) shared(...)
 \end{lstlisting}
%\end{lstlisting}

 \item Проверить все for-циклы на внутренние зависимости по данным между итерациями. Если зависимости обнаружились, использовать для защиты критических секций директиву \lstinline[language=C]{#pragma omp critical} или \lstinline[language=C]{#pragma omp atomic} (если операция атомарна), или параметр $reduction$ (предпочтительнее) или вообще отказаться от распараллеливания цикла (свой выбор необходимо обосновать).

 \item Убедиться, что получившаяся программа обладает свойством прямой совместимости с компиляторами, не поддерживающими OpenMP (для проверки этого можно скомпилировать программу без опции $-fopenmp$, в результате не должно быть сообщений об ошибках, а программа должна корректно работать).

 \item Использовать функцию SetNumThreads для изменения числа потоков. В отчете указать максимальное количество потоков.

 \item Провести эксперименты, замеряя параллельное ускорение. Привести сравнение графиков параллельного ускорения с ЛР №1 и ЛР №2.

 \item Провести эксперименты, добавив параметр $schedule$ и варьируя в экспериментах тип расписания. Исследование нужно провести для всех возможных расписаний: $static$, $dynamic$, $guided$. С 4 вариантами $chunck\_size$ равными: единице, меньше чем число потоков, равному числу потоков и больше чем число потоков. Привести сравнение параллельного ускорения при различных расписаниях с результатами п.4.

 \item Определить какой тип расписания на машине при использовании $schedule$ $auto$.

 \item Выбрать из рассмотренных в п.4 и п.5 наилучший вариант при различных $N$. Сформулировать условия, при которых наилучшие результаты получились бы при использовании других типов расписания.

 \item Найти вычислительную сложность алгоритма до и после распараллеливания, сравнить полученные результаты.

 \item Для иллюстрации того, что программа действительно распараллелилась, привести график загрузки процессора (ядер) от времени при выполнении программы при $N = N2$ для лучшего варианта распараллеливания.

 \item Построить график параллельного ускорения для точек $N < N2$ и найти значения $N$, при которых накладные расходы на распараллеливание превышают выигрыш от распараллеливания (независимо для различных типов расписания).

 \item Для лучшего результата по итогам всех экспериментов сделать еще минимум 3 эксперимента, заменив флаг $-O3$ на другие флаги оптимизации. Построить график времени выполнения от $N$.
\end{enumerate}

\textbf{Вариант:} $(6*6*9) \% 47 = 42$ - гиперболический синус с последующим возведением в квадрат, модуль тангенса, возведение в степень, сортировка выбором.

\clearpage
\section*{СРЕДА ВЫПОЛНЕНИЯ}
\addcontentsline{toc}{section}{СРЕДА ВЫПОЛНЕНИЯ}

Код собирается следующими компиляторами:
\begin{itemize}
 \setlength{\itemindent}{3em}
 \item GCC 12.2.0
\end{itemize}

Процессор: AMD Ryzen 5 3550H with Radeon Vega Mobile Gfx

Архитектура: x86\_64

Количество сокетов: 1

Количество ядер: 4

Количество потоков на ядро: 2 (дополнительные потоки выключены при замерах)

Размеры кэшей:
\begin{itemize}
 \setlength{\itemindent}{3em}
 \item L1d: 128 KiB (4 instances)
 \item L1i: 256 KiB (4 instances)
 \item L2: 2 MiB (4 instances)
 \item L3: 4 MiB (1 instance)
\end{itemize}

ОЗУ: 6 GB, без swap

ОС: Arch Linux (rolling release) Linux version 5.19.10-arch1-1

Разрядность ОС: 64 bit


\clearpage
\section*{ХОД РАБОТЫ}
\addcontentsline{toc}{section}{ХОД РАБОТЫ}
Исходный код доступен по ссылке

\url{https://github.com/bgs99/ParallelProcessing}

\clearpage

\section*{Результаты}

Замеры параллельного ускорения при использовании $schedule$ $auto$:

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task5/acceleration_par_1.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task5/acceleration_par_2.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task5/acceleration_par_4.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task5/acceleration_par_6.png}

Как видно на графиках, распараллеливание с помощью OpenMP не дает заметного ускорения по сравнению с автоматическим распараллеливанием и использованием AMD Framewave. Кроме того, в отличие от AMD Framewave, OpenMP не имеет защиты от использования большего числа потоков, чем позволяет аппаратный параллелелизм, что приводит к значительному ухудшению параллельного ускорения на 6 потоках при небольших наборах данных.

Замеры параллельного ускорения при использовании различных $schedule$ и $chunk\_size$:

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task6/acceleration_par_2.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task6/acceleration_par_4.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task6/acceleration_par_6.png}

Графики показывают ожидаемый проигрыш $dynamic$ по сравнению со $static$: в данной ЛР вычислительные сложности итераций примерно одинаковы, что нивелирует преимущества динамического разделения итераций, оставляя при этом накладные расходы.

Незначительное преимущество $guided$ по сравнению со $static$ связано, вероятно, с низкой вычислительной стоимостью цикла по сравнению со стоимостью запуска нитей: в начале выполнения $guided$ выдает большие чанки, предотвращая частые запросы новых чанков как в $dynamic$, однако при этом нити, завершившие свой чанк раньше не простаивают, как в $static$. Особенно сильно последнее заметно при превышении аппаратного параллелилизма, где разные нити, фактически, имеют различную "производительность": "лишние" нити конкурируют за ядра процессора и потому выполняются реже.

Для определения расписания по-умолчанию было написано тестовое приложение, доступное в файле $schedule.c$. Это приложение, при выполнении, выводит соответствие между нитями и итерациями.
По результатам выполнения был сделан вывод, что по умолчанию используется расписание $static$ с автоматическим размером чанка, равным количеству итераций, деленному на количество нитей.

По результатов замеров параллельного ускорения для всех значений $N$ лучшим вариантом является $guided$ расписание с $chunk\_size$, равным количеству нитей.

$static$ мог бы стать лучшим вариантом, если бы вычислительная сложность итераций была выше, но при этом оставалась равномерной.

$dynamic$ мог бы стать лучшим вариантом, если бы вычислительная сложность итераций была сильно неравномерной (например, если бы большой чанк размером в количество итераций, деленный на количество нитей мог занять больше времени, чем все остальные итерации, поделенные между остальными нитями).

До и после распараллеливания вычислительная сложность алгоритма осталась квадратичной, так как в ней используется сортировка выбором, не предполагающая тривиальное распрараллеливание посредством аннотации циклов.

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task9/time_complexity.png}

Как и в предыдущих ЛР, факт распараллеливания не виден ввиду того, что большую часть выполнения занимает нераспараллеленная квадратичная сортировка:

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task10/Percentage of CPU utilization per processor number over time (P=2, GUIDED, M).png}

Для определения значений $N$, для которых накладные расходы на распараллеливание превышают выигрыш от распараллеливания, были построены следующие графики:

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3.png}

На данном графике видо, что при использовании расписания по умолчанию выиграш заметен только при $N<= 5720$, с нестабильными результатами при $N=900$.


\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3-static-1.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3-static-M-1.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3-static-M.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3-static-M+1.png}

Как видно на данных графиках, для $static$ расписания при $M$ не превышающем аппаратный параллелелизм стабильно эффективно параллелизуются только $N=5720$, также для различных размеров чанков бывает выгодно параллелизовать также и для меньших значений $N$.


\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3-dynamic-1.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3-dynamic-M.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3-dynamic-M+1.png}

По данным графикам видно, что использование $dynamic$ расписания не является универсально выгодным ни при каких значениях $N$.

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3-guided-1.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3-guided-M-1.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3-guided-M.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task13/acc_over_thread_lab3-guided-M+1.png}

На данных графиках видно, что при использовании $guided$ расписания, как и для $static$, возможно повышение производительности при $N<=5720$, однако результаты для $N=900$ неоднозначны ввиду относительно высокого уровня погрешности.

Данные результаты объясняются тем, что при повышении количества итераций время вычисления нераспараллеленной части кода растет быстрее, чем распараллеленной, нивелируя полученный выигрыш. При этом, при низких $N$ вероятна ситуация, когда накладные расходы на параллелилизм перекрывают выигрыш, однако данная ситуация не наблюдается стабильно в этих экспериментах, так как при достаточно низких значениях $N$ значительно понижается точность измерения времени выполнения.

Дополнительно, для лучшей конфигурации расписания $schedule(guided, M)$ были проведены дополнительные замеры с различными уровнями оптимизации:

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task14/time_complexity_par_1.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task14/time_complexity_par_2.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task14/time_complexity_par_4.png}

\includegraphics[width=.8\linewidth]{../build/graphs/lab3/task14/time_complexity_par_6.png}

Как видно на данных графиках, $-O2$ оптимизирует на том же уровне, что и $-O3$.
При этом, видимо, GCC некорректно оптимизирует код ЛР на уровне $-O1$, что приводит у ухудшению производительности по сравнению с $-O0$.

\clearpage
\section*{ВЫВОД}
\addcontentsline{toc}{section}{ВЫВОД}

В результате данной работы была проведена параллелизация программы с помощью директив OpenMP, проведены замеры производительности при различных расписаниях с различными размерами чанков.

Было обнаружено, что наибольшее ускорение достигается при использовании $schedule(guided, M)$, где $M$ - количество чанков.

Также было обнаружено, что уровень оптимизации GCC $-O2$ дает результаты, сравнимые с $-O3$, а результаты $-O1$ хуже, чем $-O0$.
\end{document}
